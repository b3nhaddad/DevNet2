{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nK9MFVMaX5np"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWiYU8A8YTHL",
        "outputId": "52903167-844e-476a-f777-ca14f389b345"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.4)\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/csafrit2/plant-leaves-for-image-classification?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6.56G/6.56G [01:12<00:00, 97.3MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting model files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/csafrit2/plant-leaves-for-image-classification/versions/2\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"csafrit2/plant-leaves-for-image-classification\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TkAS36dzaDMg"
      },
      "outputs": [],
      "source": [
        "\n",
        "healthy = [\n",
        "    os.path.join(path, 'Plants_2/train/Pomegranate healthy (P9a)'),\n",
        "    os.path.join(path, 'Plants_2/train/Arjun healthy (P1b)'),\n",
        "    os.path.join(path, 'Plants_2/train/Jamun healthy (P5a)')\n",
        "]\n",
        "\n",
        "diseased = [\n",
        "    os.path.join(path, 'Plants_2/train/Pomegranate diseased (P9b)'),\n",
        "    os.path.join(path, 'Plants_2/train/Arjun diseased (P1a)'),\n",
        "    os.path.join(path, 'Plants_2/train/Jamun diseased (P5b)')\n",
        "]\n",
        "\n",
        "healthy_files = [os.path.join(h, f) for h in healthy for f in os.listdir(h) if os.path.isfile(os.path.join(h, f))]\n",
        "diseased_files = [os.path.join(d, f) for d in diseased for f in os.listdir(d) if os.path.isfile(os.path.join(d, f))]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEQBmFDSdTcB",
        "outputId": "89c4f888-b933-4554-8f26-7884b8b26f15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of healthy images: 755\n",
            "Number of diseased images: 818\n",
            "Total number of images: 1573\n"
          ]
        }
      ],
      "source": [
        "\n",
        "all_files = healthy_files + diseased_files\n",
        "labels = [0] * len(healthy_files) + [1] * len(diseased_files)\n",
        "\n",
        "print(f\"Number of healthy images: {len(healthy_files)}\")\n",
        "print(f\"Number of diseased images: {len(diseased_files)}\")\n",
        "print(f\"Total number of images: {len(all_files)}\")\n",
        "\n",
        "train_files, test_files, train_labels, test_labels = train_test_split(all_files, labels, test_size=0.3, stratify=labels, random_state=42)\n",
        "\n",
        "\n",
        "class PlantImageDataset(Dataset):\n",
        "    def __init__(self, image_files, labels, transform=None, target_size=(128, 128)):\n",
        "        self.image_files = image_files\n",
        "        self.labels = labels\n",
        "        self.target_size = target_size\n",
        "\n",
        "        self.transform = transform if transform else transforms.Compose([\n",
        "            transforms.Resize(self.target_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_files[idx]\n",
        "        image = Image.open(img_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        return image, label\n",
        "train_dataset = PlantImageDataset(train_files, train_labels, target_size=(128, 128))\n",
        "test_dataset = PlantImageDataset(test_files, test_labels, target_size=(128, 128))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0WWj_YEioG7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTxXn7geoR0e",
        "outputId": "44971aad-2ad5-43a1-f164-61bf61099cb5"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "7voEQBPeYEwb",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8749da48-73d6-4c60-b491-cb0a8b4d9a5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "CNN_model = models.resnet18(pretrained=True)\n",
        "for param in CNN_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "num_features = CNN_model.fc.in_features\n",
        "CNN_model.fc = nn.Linear(num_features, 2)\n",
        "\n",
        "CNN_model = CNN_model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(CNN_model.fc.parameters(), lr=0.001, momentum=0.9)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "def calculate_accuracy(predictions, labels):\n",
        "    _, predicted = torch.max(predictions, 1)\n",
        "    correct = (predicted == labels).sum().item()\n",
        "    return correct / len(labels)\n",
        "\n",
        "\n",
        "num_epochs = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "CNN_model = CNN_model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    CNN_model.train()\n",
        "    running_loss = 0.0\n",
        "    running_accuracy = 0.0\n",
        "\n",
        "    for images, labels in tqdm(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = CNN_model(images)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        running_accuracy += calculate_accuracy(outputs, labels)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = running_accuracy / len(train_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ypRc3m_rQ0x",
        "outputId": "a65ac54c-8f58-4588-80e0-9df1a6be77a0"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [01:00<00:00,  1.72s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.5642, Accuracy: 0.7192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [01:04<00:00,  1.83s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], Loss: 0.3496, Accuracy: 0.8603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [01:03<00:00,  1.82s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10], Loss: 0.3008, Accuracy: 0.8786\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [01:02<00:00,  1.80s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10], Loss: 0.2966, Accuracy: 0.8716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [01:04<00:00,  1.85s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10], Loss: 0.2861, Accuracy: 0.8799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [01:02<00:00,  1.80s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10], Loss: 0.2432, Accuracy: 0.9049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [01:04<00:00,  1.84s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10], Loss: 0.2464, Accuracy: 0.9005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [01:03<00:00,  1.82s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10], Loss: 0.2153, Accuracy: 0.9130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [01:03<00:00,  1.82s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10], Loss: 0.2135, Accuracy: 0.9232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [01:02<00:00,  1.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10], Loss: 0.2028, Accuracy: 0.9255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "CNN_model.eval()  # Set the model to evaluation mode\n",
        "test_loss = 0.0\n",
        "test_accuracy = 0.0\n",
        "\n",
        "# Initialize counters for healthy vs diseased\n",
        "healthy_count = 0\n",
        "diseased_count = 0\n",
        "correct_healthy = 0\n",
        "correct_diseased = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Get model outputs\n",
        "        outputs = CNN_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        test_accuracy += calculate_accuracy(outputs, labels)\n",
        "\n",
        "        # Get predicted class\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        # Count healthy and diseased predictions\n",
        "        for i in range(len(labels)):\n",
        "            if labels[i].item() == 0:  # Assuming label 0 is 'healthy'\n",
        "                healthy_count += 1\n",
        "                if predicted[i].item() == 0:\n",
        "                    correct_healthy += 1\n",
        "            elif labels[i].item() == 1:  # Assuming label 1 is 'diseased'\n",
        "                diseased_count += 1\n",
        "                if predicted[i].item() == 1:\n",
        "                    correct_diseased += 1\n",
        "\n",
        "# Calculate final metrics\n",
        "test_loss /= len(test_loader)\n",
        "test_accuracy /= len(test_loader)\n",
        "\n",
        "print(f\"Validation Loss: {test_loss:.4f}, Validation Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Healthy images: {healthy_count}, Diseased images: {diseased_count}\")\n",
        "print(f\"Correct healthy predictions: {correct_healthy}\")\n",
        "print(f\"Correct diseased predictions: {correct_diseased}\")\n",
        "print(f\"Accuracy on healthy images: {correct_healthy / healthy_count:.4f}\")\n",
        "print(f\"Accuracy on diseased images: {correct_diseased / diseased_count:.4f}\")\n",
        "print(\"Training completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UG1cRqv1UkR",
        "outputId": "c359b6b8-b8c2-40d1-b621-cf54a83b4b95"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.2097, Validation Accuracy: 0.9243\n",
            "Healthy images: 227, Diseased images: 245\n",
            "Correct healthy predictions: 214\n",
            "Correct diseased predictions: 222\n",
            "Accuracy on healthy images: 0.9427\n",
            "Accuracy on diseased images: 0.9061\n",
            "Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFDFvdUTkuHY",
        "outputId": "f88ae73e-0a22-4308-9cab-a35dcc6cd15b"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('/content/drive/My Drive/models', exist_ok=True)\n",
        "model_path = '/content/drive/My Drive/models/cnn_model_weights.pth'\n",
        "torch.save(CNN_model.state_dict(), model_path)\n"
      ],
      "metadata": {
        "id": "nMOldDyelTR2"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "# Define the transformations (same as in the dataset)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Match target size used in dataset\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Match training normalization\n",
        "])\n",
        "\n",
        "# Load the image (replace with your new image path)\n",
        "image_path = '/content/healthy.png'\n",
        "image = Image.open(image_path).convert('RGB')  # Ensure it's RGB\n",
        "\n",
        "# Apply the transformations\n",
        "image = transform(image)\n",
        "\n",
        "# Add batch dimension (model expects a batch, even for a single image)\n",
        "image = image.unsqueeze(0)  # shape will be (1, 3, 128, 128)\n",
        "\n",
        "\n",
        "# Move the model and image to the device (GPU or CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CNN_model.to(device)\n",
        "image = image.to(device)\n",
        "\n",
        "# Make the prediction\n",
        "with torch.no_grad():  # No need to track gradients for inference\n",
        "    output = CNN_model(image)\n",
        "\n",
        "# Get predicted class\n",
        "_, predicted_class = torch.max(output, 1)\n",
        "\n",
        "# Map class index to class name\n",
        "class_names = ['Healthy', 'Diseased']  # Modify if you have different classes\n",
        "predicted_label = class_names[predicted_class.item()]\n",
        "\n",
        "# Print the prediction\n",
        "print(f\"Predicted label: {predicted_label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TJ_CqlsC4yJ",
        "outputId": "44a34b15-d6e5-40d8-9c6e-009f0ec39937"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted label: Healthy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import shutil\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "# Import the CNN model from setup.py\n",
        "# Instantiate the model and set it to evaluation mode\n",
        "\n",
        "\n",
        "# Function to standardize the image\n",
        "def standardize_image(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\") #changes the color of the image\n",
        "    img_array = np.array(image)\n",
        "    mean = np.mean(img_array, axis=(0, 1))\n",
        "    std = np.std(img_array, axis=(0, 1))\n",
        "    standardized_img = (img_array - mean) / std\n",
        "    standardized_img = np.clip(standardized_img, 0, 1) #smooths image over 0 to 1\n",
        "    standardized_img = Image.fromarray((standardized_img * 255).astype(np.uint8))\n",
        "    return standardized_img\n",
        "#makes sure he image is standardized (in color, size, resolution )\n",
        "# Function to classify the standardized image\n",
        "def classify_image(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image_tensor = transform_image_for_model(image)  # Transform for model input\n",
        "    with torch.no_grad():\n",
        "        output = CNN_model(image_tensor)  # Correct way to call the model\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        label = \"Healthy\" if predicted.item() == 0 else \"Unhealthy\"\n",
        "    return label\n",
        "\n",
        "# Preprocessing: Convert the PIL image to a tensor for the model\n",
        "def transform_image_for_model(image):\n",
        "    image = image.resize((128, 128))  # Adjust size as required by the model\n",
        "    image = np.array(image).transpose((2, 0, 1))  # Rearrange to CxHxW\n",
        "    image_tensor = torch.tensor(image, dtype=torch.float32).unsqueeze(0) / 255.0  # Add batch dimension\n",
        "    return image_tensor\n",
        "\n",
        "\n",
        "# Function to handle file upload and processing\n",
        "# Function to handle file upload and processing\n",
        "def upload_file(file_path, target_directory):\n",
        "    if not os.path.exists(target_directory):\n",
        "        os.makedirs(target_directory)\n",
        "\n",
        "    target_path = os.path.join(target_directory, os.path.basename(file_path))\n",
        "\n",
        "    # Check if source and destination are the same\n",
        "    if os.path.abspath(file_path) == os.path.abspath(target_path):\n",
        "        print(f\"Source and target paths are the same: {file_path}\")\n",
        "    else:\n",
        "        shutil.copy(file_path, target_path)  # Use copy instead of move if needed\n",
        "\n",
        "    # Standardize and classify\n",
        "    #the restandardization is needed to be properly classified\n",
        "    standardized_image = standardize_image(target_path)\n",
        "    standardized_image.save(target_path)\n",
        "    print(f\"Image standardized and saved to {target_path}\")\n",
        "\n",
        "    # Classify image\n",
        "    classification = classify_image(target_path)\n",
        "    #since this is a binary model classification is either healthy or unhealthy\n",
        "    print(f\"Image classified as: {classification}\")\n",
        "\n",
        "upload_file('/content/unheal.jpg', '/content/target')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPTL-_eCJx4d",
        "outputId": "ec32e47a-ae2d-4554-9576-1436dadef53c"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image standardized and saved to /content/target/unheal.jpg\n",
            "Image classified as: Unhealthy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3o2FcLI6oZRE"
      }
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}